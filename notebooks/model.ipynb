{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import rasterio as rio\n",
    "from matplotlib import pyplot as plt\n",
    "from rasterio.plot import show\n",
    "import re\n",
    "import cv2\n",
    "import csv\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from shapely.geometry import Polygon\n",
    "from imantics import Polygons, Mask\n",
    "from scipy.ndimage.morphology import distance_transform_edt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shimash\n"
     ]
    }
   ],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"SpaceNet6 Dataset.\"\"\"\n",
    "\n",
    "class SpaceNetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_dir,\n",
    "        image_dir,\n",
    "        sar_dir,\n",
    "#         test_dir,\n",
    "        transform=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (string): Path to RGB images directory.\n",
    "            sar_dir (string): ...\n",
    "            mask_dir (string): ... \n",
    "            tile_number (string): ...\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.csv_dir = csv_dir\n",
    "        self.data = pd.read_csv(csv_dir)\n",
    "        self.image_dir = image_dir\n",
    "        self.sar_dir = sar_dir\n",
    "        self.mask_list = self.create_poly_list()\n",
    "        self.transform = transform\n",
    "        self.tile_id_list = self.tile_id_list()\n",
    "#         self.test_dir = test_dir\n",
    "#         self.test_id_list = self.test_id_list()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tile_id_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        image = self.get_rgb(idx)\n",
    "        mask = self.get_mask(idx)\n",
    "        sar = self.get_sar(idx)\n",
    "        edge = self.get_edge(idx, mask)\n",
    "        \n",
    "        if self.transform:\n",
    "            image, mask, _ = self.transform(image, mask)\n",
    "\n",
    "        return sar.float(), mask.float(), image.float(), edge.float() \n",
    "    \n",
    "    def get_sar(self, idx):\n",
    "        create_SAR_path = self.sar_dir +\"SN6_Train_AOI_11_Rotterdam_SAR-Intensity_\" + self.tile_id_list[idx][0] + \".tif\"\n",
    "        with rio.open(create_SAR_path) as lidar_dem:\n",
    "            img = np.zeros((900,900,4))\n",
    "            img[:,:,0] = lidar_dem.read(1)\n",
    "            img[:,:,1] = lidar_dem.read(2)\n",
    "            img[:,:,2] = lidar_dem.read(3)\n",
    "            img[:,:,3] = lidar_dem.read(4)\n",
    "        return torch.from_numpy(img*255/np.max(img))\n",
    "    \n",
    "    def get_rgb(self, idx):\n",
    "        create_RGB_path = self.image_dir +\"SN6_Train_AOI_11_Rotterdam_PS-RGB_\" + self.tile_id_list[idx][0] + \".tif\"\n",
    "        with rio.open(create_RGB_path) as lidar_dem:\n",
    "            img = np.zeros((900,900,3))\n",
    "            img[:,:,0] = lidar_dem.read(1)\n",
    "            img[:,:,1] = lidar_dem.read(2)\n",
    "            img[:,:,2] = lidar_dem.read(3)\n",
    "        return torch.from_numpy(img)\n",
    "\n",
    "    def get_mask(self, idx):\n",
    "        return self.generate_mask(self.mask_list[idx][1], self.mask_list[idx][2])\n",
    "         \n",
    "    def generate_mask(self, start, end):\n",
    "        mask_img = Image.new('1', (900, 900), 0)\n",
    "        poly = ImageDraw.Draw(mask_img)\n",
    "        for i in range(start,end+1):\n",
    "            row = self.data.loc[i,'PolygonWKT_Pix']\n",
    "            expression = re.findall(\"[-+]?[.]?[\\d]+(?:,\\d\\d\\d)*[\\.]?\\d*(?:[eE][-+]?\\d+)?\", row)\n",
    "            tup = (float(expression[0]), float(expression[1]))\n",
    "            for i in range(2,len(expression),2):\n",
    "                temp = (float(expression[i]),float(expression[i+1]))\n",
    "                tup = tup + temp\n",
    "            poly.polygon(tup, outline = 1, fill = 1)\n",
    "        mask = np.array(mask_img)\n",
    "        mask = torch.from_numpy(mask).float()\n",
    "        return mask \n",
    "    \n",
    "    def onehot_to_binary_edges(self, mask, radius):\n",
    "        mask = mask.numpy()\n",
    "        if radius < 0:\n",
    "            return mask\n",
    "        mask = mask.astype(np.uint8)\n",
    "        # We need to pad the borders for boundary conditions\n",
    "        mask_pad = np.pad(mask, ((1, 1), (1, 1)), mode='constant', constant_values=0)\n",
    "        edgemap = np.zeros(mask.shape)\n",
    "        dist = distance_transform_edt(mask_pad)\n",
    "        #print(dist)\n",
    "        dist = dist[1:-1, 1:-1]\n",
    "        dist[dist > radius] = 0\n",
    "        edgemap += dist\n",
    "#         edgemap = np.expand_dims(edgemap, axis=0)    \n",
    "#         edgemap = (edgemap > 0).astype(np.uint8)\n",
    "        return edgemap\n",
    "    \n",
    "    def get_edge(self, idx, mask):\n",
    "        _edgemap = mask\n",
    "        _edgemap = self.onehot_to_binary_edges(_edgemap, 1)\n",
    "        edgemap = torch.from_numpy(_edgemap).float()\n",
    "        return edgemap\n",
    "    \n",
    "    def create_poly_list(self):\n",
    "        pointer = 0\n",
    "        poly_index_list = []\n",
    "        while pointer < len(self.data):\n",
    "            start = pointer\n",
    "            end = pointer\n",
    "            tile_number_st = self.data['ImageId'][pointer].rfind('_')\n",
    "            tile_number = self.data['ImageId'][pointer][tile_number_st+1:]\n",
    "            while (pointer+2) < len(self.data) and (self.data['TileBuildingId'][pointer] < self.data['TileBuildingId'][pointer+1]):\n",
    "                end +=1\n",
    "                pointer +=1\n",
    "            poly_index_list.append([tile_number, start, end, self.data['ImageId'][pointer]])\n",
    "            pointer +=1\n",
    "        return poly_index_list\n",
    "        \n",
    "    def random_rotation(self, image, mask):\n",
    "        orient = np.random.randint(0, 4)\n",
    "        image = np.rot90(image, orient)\n",
    "        mask = np.rot90(mask, orient)\n",
    "        sar = np.rot90(sar, orient)\n",
    "        return image, mask, sar, orient\n",
    "\n",
    "    def tile_id_list(self):\n",
    "        tile_list = []\n",
    "        csv_list = self.create_poly_list()\n",
    "        for tile in range(len(csv_list)):\n",
    "            tile_list.append([csv_list[tile][3], csv_list[tile][0]])\n",
    "        return tile_list\n",
    "    \n",
    "    def create_output_csv(self):\n",
    "        with open('solution.csv', 'w') as csv_file:\n",
    "            fieldnames = ['ImageId','PolygonWKT_Pix','Confidence']\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "    \n",
    "    def csv_add_newline(self, image_id, polygons, score):\n",
    "        with open('solution.csv', 'a', newline='') as csv_file:\n",
    "            fieldnames = ['ImageId','PolygonWKT_Pix','Confidence']\n",
    "            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "            writer.writerow({'ImageId': image_id, 'PolygonWKT_Pix': polygons, 'Confidence': score})\n",
    "    \n",
    "    def create_polygon(self, idx, mask):\n",
    "        mask = mask.numpy()\n",
    "        binary_mask = mask.astype(np.uint8)\n",
    "        contours, hierarchy = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        for i in range (len(contours)):\n",
    "            points = []\n",
    "            contour = np.squeeze(contours[i])\n",
    "            for j in range(len(contour)):\n",
    "                points.append((contour[j][0], contour[j][1]))\n",
    "            if contour.shape[0] >= 3:\n",
    "                polygon = Polygon(points)   \n",
    "            #add a newline to csv!   \n",
    "            image_id = self.mask_list[idx][3]\n",
    "            self.csv_add_newline(image_id, polygon.wkt, 1.0)\n",
    "            #print(polygon.wkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_dir = \"data/train/AOI_11_Rotterdam/geojson_buildings/\"\n",
    "img_dir = \"data/train/AOI_11_Rotterdam/PS-RGB/\"\n",
    "sar_dir = \"data/train/AOI_11_Rotterdam/SAR-Intensity/\"\n",
    "out_dir = \"data/train/AOI_11_Rotterdam/gt_masks/\"\n",
    "csv_dir = \"data/train/AOI_11_Rotterdam/SummaryData/SN6_Train_AOI_11_Rotterdam_Buildings.csv\"\n",
    "\n",
    "test_space = SpaceNetDataset(csv_dir=csv_dir, image_dir=img_dir, sar_dir=sar_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar, mask, rgb, edge =  test_space[0]\n",
    "sar_1, mask_1, rgb_1, edge_1 =  test_space[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([224, 224, 4])\n"
     ]
    }
   ],
   "source": [
    "# rgb_test = rgb.unsqueeze(0).view((1,3,900,900))\n",
    "# rgb_test_1 = rgb_1.unsqueeze(0).view((1,3,900,900))\n",
    "rgb_test = sar\n",
    "rgb_test_1 = sar_1\n",
    "rgb_test = rgb_test.cpu().numpy()\n",
    "rgb_test_1 = rgb_test_1.cpu().numpy()\n",
    "\n",
    "rgb_resized = cv2.resize(rgb_test, (224, 224)) \n",
    "rgb_1_resized = cv2.resize(rgb_test_1, (224, 224)) \n",
    "\n",
    "rgb_resized = torch.from_numpy(rgb_resized).float()\n",
    "rgb_1_resized = torch.from_numpy(rgb_1_resized).float()\n",
    "\n",
    "rgb_out = rgb_resized.unsqueeze(0).view((1,4,224,224))\n",
    "rgb_out_1 = rgb_1_resized.unsqueeze(0).view((1,4,224,224))\n",
    "\n",
    "print(rgb_resized.shape)\n",
    "test = torch.cat((rgb_out, rgb_out_1),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-577309c931d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mFpc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_mask_boundary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-577309c931d9>\u001b[0m in \u001b[0;36meval_mask_boundary\u001b[0;34m(seg_mask, gt_mask, num_classes, num_proc, bound_th)\u001b[0m\n\u001b[1;32m     28\u001b[0m                  \u001b[0mgt_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                  bound_th) \n\u001b[0;32m---> 30\u001b[0;31m                  for i in range(batch_size)]\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_eval_boundary_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-577309c931d9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m                  \u001b[0mgt_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                  bound_th) \n\u001b[0;32m---> 30\u001b[0;31m                  for i in range(batch_size)]\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_eval_boundary_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\" Utilities for computing, reading and saving benchmark evaluation.\"\"\"\n",
    "\n",
    "def eval_mask_boundary(seg_mask,gt_mask,num_classes=2,num_proc=10,bound_th=0.008):\n",
    "    \"\"\"\n",
    "    Compute F score for a segmentation mask\n",
    "\n",
    "    Arguments:\n",
    "        seg_mask (ndarray): segmentation mask prediction\n",
    "        gt_mask (ndarray): segmentation mask ground truth\n",
    "        num_classes (int): number of classes\n",
    "\n",
    "    Returns:\n",
    "        F (float): mean F score across all classes\n",
    "        Fpc (listof float): F score per class\n",
    "    \"\"\"\n",
    "    p = Pool(processes=num_proc)\n",
    "    batch_size = seg_mask.shape[0]\n",
    "    \n",
    "    Fpc = np.zeros(num_classes)\n",
    "    Fc = np.zeros(num_classes)\n",
    "    for class_id in tqdm(range(num_classes)):\n",
    "        args = [((seg_mask[i] == class_id).astype(np.uint8), \n",
    "                 (gt_mask[i] == class_id).astype(np.uint8),\n",
    "                 gt_mask[i] == 255,\n",
    "                 bound_th) \n",
    "                 for i in range(batch_size)]\n",
    "        temp = p.map(db_eval_boundary_wrapper, args)\n",
    "        temp = np.array(temp)\n",
    "        Fs = temp[:,0]\n",
    "        _valid = ~np.isnan(Fs)\n",
    "        Fc[class_id] = np.sum(_valid)\n",
    "        Fs[np.isnan(Fs)] = 0\n",
    "        Fpc[class_id] = sum(Fs)\n",
    "    return Fpc, Fc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#def db_eval_boundary_wrapper_wrapper(args):\n",
    "#    seg_mask, gt_mask, class_id, batch_size, Fpc = args\n",
    "#    print(\"class_id:\" + str(class_id))\n",
    "#    p = Pool(processes=10)\n",
    "#    args = [((seg_mask[i] == class_id).astype(np.uint8), \n",
    "#             (gt_mask[i] == class_id).astype(np.uint8)) \n",
    "#             for i in range(batch_size)]\n",
    "#    Fs = p.map(db_eval_boundary_wrapper, args)\n",
    "#    Fpc[class_id] = sum(Fs)\n",
    "#    return\n",
    "\n",
    "# def db_eval_boundary_wrapper(args):\n",
    "#     foreground_mask, gt_mask, ignore, bound_th = args\n",
    "#     return db_eval_boundary(foreground_mask, gt_mask,ignore, bound_th)\n",
    "\n",
    "def db_eval_boundary(foreground_mask,gt_mask, ignore_mask,bound_th=0.008):\n",
    "\t\"\"\"\n",
    "\tCompute mean,recall and decay from per-frame evaluation.\n",
    "\tCalculates precision/recall for boundaries between foreground_mask and\n",
    "\tgt_mask using morphological operators to speed it up.\n",
    "\n",
    "\tArguments:\n",
    "\t\tforeground_mask (ndarray): binary segmentation image.\n",
    "\t\tgt_mask         (ndarray): binary annotated image.\n",
    "\n",
    "\tReturns:\n",
    "\t\tF (float): boundaries F-measure\n",
    "\t\tP (float): boundaries precision\n",
    "\t\tR (float): boundaries recall\n",
    "\t\"\"\"\n",
    "\tassert np.atleast_3d(foreground_mask).shape[2] == 1\n",
    "\n",
    "\tbound_pix = bound_th if bound_th >= 1 else \\\n",
    "\t\t\tnp.ceil(bound_th*np.linalg.norm(foreground_mask.shape))\n",
    "\n",
    "\t#print(bound_pix)\n",
    "\t#print(gt.shape)\n",
    "\t#print(np.unique(gt))\n",
    "\tforeground_mask[ignore_mask] = 0\n",
    "\tgt_mask[ignore_mask] = 0\n",
    "\n",
    "\t# Get the pixel boundaries of both masks\n",
    "\tfg_boundary = seg2bmap(foreground_mask);\n",
    "\tgt_boundary = seg2bmap(gt_mask);\n",
    "\n",
    "\tfrom skimage.morphology import binary_dilation,disk\n",
    "\n",
    "\tfg_dil = binary_dilation(fg_boundary,disk(bound_pix))\n",
    "\tgt_dil = binary_dilation(gt_boundary,disk(bound_pix))\n",
    "\n",
    "\t# Get the intersection\n",
    "\tgt_match = gt_boundary * fg_dil\n",
    "\tfg_match = fg_boundary * gt_dil\n",
    "\n",
    "\t# Area of the intersection\n",
    "\tn_fg     = np.sum(fg_boundary)\n",
    "\tn_gt     = np.sum(gt_boundary)\n",
    "\n",
    "\t#% Compute precision and recall\n",
    "\tif n_fg == 0 and  n_gt > 0:\n",
    "\t\tprecision = 1\n",
    "\t\trecall = 0\n",
    "\telif n_fg > 0 and n_gt == 0:\n",
    "\t\tprecision = 0\n",
    "\t\trecall = 1\n",
    "\telif n_fg == 0  and n_gt == 0:\n",
    "\t\tprecision = 1\n",
    "\t\trecall = 1\n",
    "\telse:\n",
    "\t\tprecision = np.sum(fg_match)/float(n_fg)\n",
    "\t\trecall    = np.sum(gt_match)/float(n_gt)\n",
    "\n",
    "\t# Compute F measure\n",
    "\tif precision + recall == 0:\n",
    "\t\tF = 0\n",
    "\telse:\n",
    "\t\tF = 2*precision*recall/(precision+recall);\n",
    "\n",
    "\treturn F, precision\n",
    "\n",
    "#checked\n",
    "def seg2bmap(seg,width=None,height=None):\n",
    "\t\"\"\"\n",
    "\tFrom a segmentation, compute a binary boundary map with 1 pixel wide\n",
    "\tboundaries.  The boundary pixels are offset by 1/2 pixel towards the\n",
    "\torigin from the actual segment boundary.\n",
    "\n",
    "\tArguments:\n",
    "\t\tseg     : Segments labeled from 1..k.\n",
    "\t\twidth\t  :\tWidth of desired bmap  <= seg.shape[1]\n",
    "\t\theight  :\tHeight of desired bmap <= seg.shape[0]\n",
    "\n",
    "\tReturns:\n",
    "\t\tbmap (ndarray):\tBinary boundary map.\n",
    "\n",
    "\t David Martin <dmartin@eecs.berkeley.edu>\n",
    "\t January 2003\n",
    " \"\"\"\n",
    "\n",
    "\tseg = seg.astype(np.bool)\n",
    "\tseg[seg>0] = 1\n",
    "\n",
    "\tassert np.atleast_3d(seg).shape[2] == 1\n",
    "\n",
    "\twidth  = seg.shape[1] if width  is None else width\n",
    "\theight = seg.shape[0] if height is None else height\n",
    "\n",
    "\th,w = seg.shape[:2]\n",
    "\n",
    "\tar1 = float(width) / float(height)\n",
    "\tar2 = float(w) / float(h)\n",
    "\n",
    "\tassert not (width>w | height>h | abs(ar1-ar2)>0.01),\\\n",
    "\t\t\t'Can''t convert %dx%d seg to %dx%d bmap.'%(w,h,width,height)\n",
    "\n",
    "\te  = np.zeros_like(seg)\n",
    "\ts  = np.zeros_like(seg)\n",
    "\tse = np.zeros_like(seg)\n",
    "\n",
    "\te[:,:-1]    = seg[:,1:]\n",
    "\ts[:-1,:]    = seg[1:,:]\n",
    "\tse[:-1,:-1] = seg[1:,1:]\n",
    "\n",
    "\tb        = seg^e | seg^s | seg^se\n",
    "\tb[-1,:]  = seg[-1,:]^e[-1,:]\n",
    "\tb[:,-1]  = seg[:,-1]^s[:,-1]\n",
    "\tb[-1,-1] = 0\n",
    "\n",
    "\tif w == width and h == height:\n",
    "\t\tbmap = b\n",
    "\telse:\n",
    "\t\tbmap = np.zeros((height,width))\n",
    "\t\tfor x in range(w):\n",
    "\t\t\tfor y in range(h):\n",
    "\t\t\t\tif b[y,x]:\n",
    "\t\t\t\t\tj = 1+floor((y-1)+height / h)\n",
    "\t\t\t\t\ti = 1+floor((x-1)+width  / h)\n",
    "\t\t\t\t\tbmap[j,i] = 1;\n",
    "\n",
    "\treturn bmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "\n",
    "    IMMUTABLE = '__immutable__'\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__[AttrDict.IMMUTABLE] = False\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if name in self.__dict__:\n",
    "            return self.__dict__[name]\n",
    "        elif name in self:\n",
    "            return self[name]\n",
    "        else:\n",
    "            raise AttributeError(name)\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if not self.__dict__[AttrDict.IMMUTABLE]:\n",
    "            if name in self.__dict__:\n",
    "                self.__dict__[name] = value\n",
    "            else:\n",
    "                self[name] = value\n",
    "        else:\n",
    "            raise AttributeError(\n",
    "                'Attempted to set \"{}\" to \"{}\", but AttrDict is immutable'.\n",
    "                format(name, value)\n",
    "            )\n",
    "\n",
    "    def immutable(self, is_immutable):\n",
    "        \"\"\"Set immutability to is_immutable and recursively apply the setting\n",
    "        to all nested AttrDicts.\n",
    "        \"\"\"\n",
    "        self.__dict__[AttrDict.IMMUTABLE] = is_immutable\n",
    "        # Recursively set immutable state\n",
    "        for v in self.__dict__.values():\n",
    "            if isinstance(v, AttrDict):\n",
    "                v.immutable(is_immutable)\n",
    "        for v in self.values():\n",
    "            if isinstance(v, AttrDict):\n",
    "                v.immutable(is_immutable)\n",
    "\n",
    "    def is_immutable(self):\n",
    "        return self.__dict__[AttrDict.IMMUTABLE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--maxSkip'], dest='maxSkip', nargs=None, const=None, default=0, type=<class 'int'>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "# Argument Parser\n",
    "parser = argparse.ArgumentParser(description='GSCNN')\n",
    "parser.add_argument('--lr', type=float, default=0.01)\n",
    "parser.add_argument('--arch', type=str, default='network.gscnn.GSCNN')\n",
    "parser.add_argument('--dataset', type=str, default='cityscapes')\n",
    "parser.add_argument('--cv', type=int, default=0,\n",
    "                    help='cross validation split')\n",
    "parser.add_argument('--joint_edgeseg_loss', action='store_true', default=True,\n",
    "                    help='joint loss')\n",
    "parser.add_argument('--img_wt_loss', action='store_true', default=False,\n",
    "                    help='per-image class-weighted loss')\n",
    "parser.add_argument('--batch_weighting', action='store_true', default=False,\n",
    "                    help='Batch weighting for class')\n",
    "parser.add_argument('--eval_thresholds', type=str, default='0.0005,0.001875,0.00375,0.005',\n",
    "                    help='Thresholds for boundary evaluation')\n",
    "parser.add_argument('--rescale', type=float, default=1.0,\n",
    "                    help='Rescaled LR Rate')\n",
    "parser.add_argument('--repoly', type=float, default=1.5,\n",
    "                    help='Rescaled Poly')\n",
    "\n",
    "parser.add_argument('--edge_weight', type=float, default=1.0,\n",
    "                    help='Edge loss weight for joint loss')\n",
    "parser.add_argument('--seg_weight', type=float, default=1.0,\n",
    "                    help='Segmentation loss weight for joint loss')\n",
    "parser.add_argument('--att_weight', type=float, default=1.0,\n",
    "                    help='Attention loss weight for joint loss')\n",
    "parser.add_argument('--dual_weight', type=float, default=1.0,\n",
    "                    help='Dual loss weight for joint loss')\n",
    "\n",
    "parser.add_argument('--evaluate', action='store_true', default=False)\n",
    "\n",
    "parser.add_argument(\"--local_rank\", default=0, type=int)\n",
    "\n",
    "parser.add_argument('--sgd', action='store_true', default=True)\n",
    "parser.add_argument('--sgd_finetuned',action='store_true',default=False)\n",
    "parser.add_argument('--adam', action='store_true', default=False)\n",
    "parser.add_argument('--amsgrad', action='store_true', default=False)\n",
    "\n",
    "parser.add_argument('--trunk', type=str, default='resnet101',\n",
    "                    help='trunk model, can be: resnet101 (default), resnet50')\n",
    "parser.add_argument('--max_epoch', type=int, default=175)\n",
    "parser.add_argument('--start_epoch', type=int, default=0)\n",
    "parser.add_argument('--color_aug', type=float,\n",
    "                    default=0.25, help='level of color augmentation')\n",
    "parser.add_argument('--rotate', type=float,\n",
    "                    default=0, help='rotation')\n",
    "parser.add_argument('--gblur', action='store_true', default=True)\n",
    "parser.add_argument('--bblur', action='store_true', default=False) \n",
    "parser.add_argument('--lr_schedule', type=str, default='poly',\n",
    "                    help='name of lr schedule: poly')\n",
    "parser.add_argument('--poly_exp', type=float, default=1.0,\n",
    "                    help='polynomial LR exponent')\n",
    "parser.add_argument('--bs_mult', type=int, default=1)\n",
    "parser.add_argument('--bs_mult_val', type=int, default=2)\n",
    "parser.add_argument('--crop_size', type=int, default=720,\n",
    "                    help='training crop size')\n",
    "parser.add_argument('--pre_size', type=int, default=None,\n",
    "                    help='resize image shorter edge to this before augmentation')\n",
    "parser.add_argument('--scale_min', type=float, default=0.5,\n",
    "                    help='dynamically scale training images down to this size')\n",
    "parser.add_argument('--scale_max', type=float, default=2.0,\n",
    "                    help='dynamically scale training images up to this size')\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-4)\n",
    "parser.add_argument('--momentum', type=float, default=0.9)\n",
    "parser.add_argument('--snapshot', type=str, default=None)\n",
    "parser.add_argument('--restore_optimizer', action='store_true', default=False)\n",
    "parser.add_argument('--exp', type=str, default='default',\n",
    "                    help='experiment directory name')\n",
    "parser.add_argument('--tb_tag', type=str, default='',\n",
    "                    help='add tag to tb dir')\n",
    "parser.add_argument('--ckpt', type=str, default='logs/ckpt')\n",
    "parser.add_argument('--tb_path', type=str, default='logs/tb')\n",
    "parser.add_argument('--syncbn', action='store_true', default=False,\n",
    "                    help='Synchronized BN')\n",
    "parser.add_argument('--dump_augmentation_images', action='store_true', default=False,\n",
    "                    help='Synchronized BN')\n",
    "parser.add_argument('--test_mode', action='store_true', default=False,\n",
    "                    help='minimum testing (1 epoch run ) to verify nothing failed')\n",
    "parser.add_argument('-wb', '--wt_bound', type=float, default=1.0)\n",
    "parser.add_argument('--maxSkip', type=int, default=0)\n",
    "# args = parser.parse_args()\n",
    "# args.best_record = {'epoch': -1, 'iter': 0, 'val_loss': 1e10, 'acc': 0,\n",
    "#                         'acc_cls': 0, 'mean_iu': 0, 'fwavacc': 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copyright (C) 2019 NVIDIA Corporation.  All rights reserved.\n",
    "Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n",
    "\n",
    "# Code adapted from:\n",
    "# https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb\n",
    "#\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2016 Eric Jang\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_grad_mag(E, cuda=False):\n",
    "    E_ = convTri(E, 4, cuda)\n",
    "    Ox, Oy = numerical_gradients_2d(E_, cuda)\n",
    "    mag = torch.sqrt(torch.mul(Ox,Ox) + torch.mul(Oy,Oy) + 1e-6)\n",
    "    mag = mag / mag.max();\n",
    "\n",
    "    return mag\n",
    "\n",
    "\n",
    "def perturbate_input_(input, n_elements=200):\n",
    "    N, C, H, W = input.shape\n",
    "    assert N == 1\n",
    "    c_ = np.random.random_integers(0, C - 1, n_elements)\n",
    "    h_ = np.random.random_integers(0, H - 1, n_elements)\n",
    "    w_ = np.random.random_integers(0, W - 1, n_elements)\n",
    "    for c_idx in c_:\n",
    "        for h_idx in h_:\n",
    "            for w_idx in w_:\n",
    "                input[0, c_idx, h_idx, w_idx] = 1\n",
    "    return input\n",
    "\n",
    "def _sample_gumbel(shape, eps=1e-10):\n",
    "    \"\"\"\n",
    "    Sample from Gumbel(0, 1)\n",
    "\n",
    "    based on\n",
    "    https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb ,\n",
    "    (MIT license)\n",
    "    \"\"\"\n",
    "    U = torch.rand(shape).cuda()\n",
    "    return - torch.log(eps - torch.log(U + eps))\n",
    "\n",
    "\n",
    "def _gumbel_softmax_sample(logits, tau=1, eps=1e-10):\n",
    "    \"\"\"\n",
    "    Draw a sample from the Gumbel-Softmax distribution\n",
    "\n",
    "    based on\n",
    "    https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb\n",
    "    (MIT license)\n",
    "    \"\"\"\n",
    "    assert logits.dim() == 3\n",
    "    gumbel_noise = _sample_gumbel(logits.size(), eps=eps)\n",
    "    y = logits + gumbel_noise\n",
    "    return F.softmax(y / tau, 1)\n",
    "\n",
    "\n",
    "def _one_hot_embedding(labels, num_classes):\n",
    "    \"\"\"Embedding labels to one-hot form.\n",
    "\n",
    "    Args:\n",
    "      labels: (LongTensor) class labels, sized [N,].\n",
    "      num_classes: (int) number of classes.\n",
    "\n",
    "    Returns:\n",
    "      (tensor) encoded labels, sized [N, #classes].\n",
    "    \"\"\"\n",
    "\n",
    "    y = torch.eye(num_classes).cuda()\n",
    "    return y[labels].permute(0,3,1,2)\n",
    "\n",
    "class DualTaskLoss(nn.Module):\n",
    "    def __init__(self, cuda=False):\n",
    "        super(DualTaskLoss, self).__init__()\n",
    "        self._cuda = cuda\n",
    "        return\n",
    "\n",
    "    def forward(self, input_logits, gts, ignore_pixel=255):\n",
    "        \"\"\"\n",
    "        :param input_logits: NxCxHxW\n",
    "        :param gt_semantic_masks: NxCxHxW\n",
    "        :return: final loss\n",
    "        \"\"\"\n",
    "        N, C, H, W = input_logits.shape\n",
    "        th = 1e-8  # 1e-10\n",
    "        eps = 1e-10\n",
    "        ignore_mask = (gts == ignore_pixel).detach()\n",
    "        input_logits = torch.where(ignore_mask.view(N, 1, H, W).expand(N, 19, H, W),\n",
    "                                   torch.zeros(N,C,H,W).cuda(),\n",
    "                                   input_logits)\n",
    "        gt_semantic_masks = gts.detach()\n",
    "        gt_semantic_masks = torch.where(ignore_mask, torch.zeros(N,H,W).long().cuda(), gt_semantic_masks)\n",
    "        gt_semantic_masks = _one_hot_embedding(gt_semantic_masks, 19).detach()\n",
    "\n",
    "        g = _gumbel_softmax_sample(input_logits.view(N, C, -1), tau=0.5)\n",
    "        g = g.reshape((N, C, H, W))\n",
    "        g = compute_grad_mag(g, cuda=self._cuda)\n",
    " \n",
    "        g_hat = compute_grad_mag(gt_semantic_masks, cuda=self._cuda)\n",
    "\n",
    "        g = g.view(N, -1)\n",
    "        g_hat = g_hat.view(N, -1)\n",
    "        loss_ewise = F.l1_loss(g, g_hat, reduction='none', reduce=False)\n",
    "\n",
    "        p_plus_g_mask = (g >= th).detach().float()\n",
    "        loss_p_plus_g = torch.sum(loss_ewise * p_plus_g_mask) / (torch.sum(p_plus_g_mask) + eps)\n",
    "\n",
    "        p_plus_g_hat_mask = (g_hat >= th).detach().float()\n",
    "        loss_p_plus_g_hat = torch.sum(loss_ewise * p_plus_g_hat_mask) / (torch.sum(p_plus_g_hat_mask) + eps)\n",
    "\n",
    "        total_loss = 0.5 * loss_p_plus_g + 0.5 * loss_p_plus_g_hat\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointEdgeSegLoss(nn.Module):\n",
    "    def __init__(self, classes=2, weight=None, reduction='mean', ignore_index=255,\n",
    "                 norm=False, upper_bound=1.0, mode='train', \n",
    "                 edge_weight=1, seg_weight=1, att_weight=1, dual_weight=1, edge='none'):\n",
    "        super(JointEdgeSegLoss, self).__init__()\n",
    "        self.num_classes = classes\n",
    "        if mode == 'train':\n",
    "            self.seg_loss = ImageBasedCrossEntropyLoss2d(\n",
    "                    classes=classes, ignore_index=ignore_index, upper_bound=upper_bound).cuda()\n",
    "        elif mode == 'val':\n",
    "            self.seg_loss = CrossEntropyLoss2d(size_average=True,\n",
    "                                               ignore_index=ignore_index).cuda()\n",
    "\n",
    "        self.edge_weight = edge_weight\n",
    "        self.seg_weight = seg_weight\n",
    "        self.att_weight = att_weight\n",
    "        self.dual_weight = dual_weight\n",
    "\n",
    "        self.dual_task = DualTaskLoss()\n",
    "\n",
    "    def bce2d(self, input, target):\n",
    "        n, c, h, w = input.size()\n",
    "    \n",
    "        log_p = input.transpose(1, 2).transpose(2, 3).contiguous().view(1, -1)\n",
    "        target_t = target.transpose(1, 2).transpose(2, 3).contiguous().view(1, -1)\n",
    "        target_trans = target_t.clone()\n",
    "\n",
    "        pos_index = (target_t ==1)\n",
    "        neg_index = (target_t ==0)\n",
    "        ignore_index=(target_t >1)\n",
    "\n",
    "        target_trans[pos_index] = 1\n",
    "        target_trans[neg_index] = 0\n",
    "\n",
    "        pos_index = pos_index.data.cpu().numpy().astype(bool)\n",
    "        neg_index = neg_index.data.cpu().numpy().astype(bool)\n",
    "        ignore_index=ignore_index.data.cpu().numpy().astype(bool)\n",
    "\n",
    "        weight = torch.Tensor(log_p.size()).fill_(0)\n",
    "        weight = weight.numpy()\n",
    "        pos_num = pos_index.sum()\n",
    "        neg_num = neg_index.sum()\n",
    "        sum_num = pos_num + neg_num\n",
    "        weight[pos_index] = neg_num*1.0 / sum_num\n",
    "        weight[neg_index] = pos_num*1.0 / sum_num\n",
    "\n",
    "        weight[ignore_index] = 0\n",
    "\n",
    "        weight = torch.from_numpy(weight)\n",
    "        weight = weight.cuda()\n",
    "        loss = F.binary_cross_entropy_with_logits(log_p, target_t, weight, size_average=True)\n",
    "        return loss\n",
    "\n",
    "    def edge_attention(self, input, target, edge):\n",
    "        n, c, h, w = input.size()\n",
    "        filler = torch.ones_like(target) * 255\n",
    "        return self.seg_loss(input, \n",
    "                             torch.where(edge.max(1)[0] > 0.8, target, filler))\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        segin, edgein = inputs\n",
    "        segmask, edgemask = targets\n",
    "\n",
    "        losses = {}\n",
    "\n",
    "        losses['seg_loss'] = self.seg_weight * self.seg_loss(segin, segmask)\n",
    "        losses['edge_loss'] = self.edge_weight * 20 * self.bce2d(edgein, edgemask)\n",
    "        losses['att_loss'] = self.att_weight * self.edge_attention(segin, segmask, edgein)\n",
    "        losses['dual_loss'] = self.dual_weight * self.dual_task(segin, segmask)\n",
    "              \n",
    "        return losses\n",
    "\n",
    "#Img Weighted Loss\n",
    "class ImageBasedCrossEntropyLoss2d(nn.Module):\n",
    "\n",
    "    def __init__(self, classes=2, weight=None, size_average=True, ignore_index=255,\n",
    "                 norm=False, upper_bound=1.0):\n",
    "        super(ImageBasedCrossEntropyLoss2d, self).__init__()\n",
    "        logging.info(\"Using Per Image based weighted loss\")\n",
    "        self.num_classes = classes\n",
    "        self.nll_loss = nn.NLLLoss2d(weight, size_average, ignore_index)\n",
    "        self.norm = norm\n",
    "        self.upper_bound = upper_bound\n",
    "        self.batch_weights = cfg.BATCH_WEIGHTING\n",
    "\n",
    "    def calculateWeights(self, target):\n",
    "        hist = np.histogram(target.flatten(), range(\n",
    "            self.num_classes + 1), normed=True)[0]\n",
    "        if self.norm:\n",
    "            hist = ((hist != 0) * self.upper_bound * (1 / hist)) + 1\n",
    "        else:\n",
    "            hist = ((hist != 0) * self.upper_bound * (1 - hist)) + 1\n",
    "        return hist\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        target_cpu = targets.data.cpu().numpy()\n",
    "        if self.batch_weights:\n",
    "            weights = self.calculateWeights(target_cpu)\n",
    "            self.nll_loss.weight = torch.Tensor(weights).cuda()\n",
    "\n",
    "        loss = 0.0\n",
    "        for i in range(0, inputs.shape[0]):\n",
    "            if not self.batch_weights:\n",
    "                weights = self.calculateWeights(target_cpu[i])\n",
    "                self.nll_loss.weight = torch.Tensor(weights).cuda()\n",
    "            \n",
    "            loss += self.nll_loss(F.log_softmax(inputs[i].unsqueeze(0)),\n",
    "                                          targets[i].unsqueeze(0))\n",
    "        return loss\n",
    "\n",
    "\n",
    "#Cross Entroply NLL Loss\n",
    "class CrossEntropyLoss2d(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True, ignore_index=255):\n",
    "        super(CrossEntropyLoss2d, self).__init__()\n",
    "        logging.info(\"Using Cross Entropy Loss\")\n",
    "        self.nll_loss = nn.NLLLoss2d(weight, size_average, ignore_index)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        return self.nll_loss(F.log_softmax(inputs), targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import copy\n",
    "import six\n",
    "import os.path as osp\n",
    "\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "\n",
    "\n",
    "\n",
    "__C = AttrDict()\n",
    "# Consumers can get config by:\n",
    "# from fast_rcnn_config import cfg\n",
    "cfg = __C\n",
    "__C.EPOCH = 0\n",
    "__C.CLASS_UNIFORM_PCT=0.0\n",
    "__C.BATCH_WEIGHTING=False\n",
    "__C.BORDER_WINDOW=1\n",
    "__C.REDUCE_BORDER_EPOCH= -1\n",
    "__C.STRICTBORDERCLASS= None\n",
    "\n",
    "__C.DATASET =AttrDict()\n",
    "# __C.DATASET.CITYSCAPES_DIR='/home/username/data/cityscapes'\n",
    "__C.DATASET.CV_SPLITS=3\n",
    "\n",
    "__C.MODEL = AttrDict()\n",
    "__C.MODEL.BN = 'regularnorm'\n",
    "__C.MODEL.BNFUNC = torch.nn.BatchNorm2d\n",
    "__C.MODEL.BIGMEMORY = False\n",
    "\n",
    "def assert_and_infer_cfg(args, make_immutable=True):\n",
    "    \"\"\"Call this function in your script after you have finished setting all cfg\n",
    "    values that are necessary (e.g., merging a config from a file, merging\n",
    "    command line config options, etc.). By default, this function will also\n",
    "    mark the global cfg as immutable to prevent changing the global cfg settings\n",
    "    during script execution (which can lead to hard to debug errors or code\n",
    "    that's harder to understand than is necessary).\n",
    "    \"\"\"\n",
    "\n",
    "    if args.batch_weighting:\n",
    "        __C.BATCH_WEIGHTING=True\n",
    "\n",
    "    if args.syncbn:\n",
    "        import encoding\n",
    "        __C.MODEL.BN = 'syncnorm'\n",
    "        __C.MODEL.BNFUNC = encoding.nn.BatchNorm2d\n",
    "    else:\n",
    "        __C.MODEL.BNFUNC = torch.nn.BatchNorm2d\n",
    "        print('Using regular batch norm')\n",
    "\n",
    "    if make_immutable:\n",
    "        cfg.immutable(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Norm2d(in_channels):\n",
    "    \"\"\"\n",
    "    Custom Norm Function to allow flexible switching\n",
    "    \"\"\"\n",
    "    layer = getattr(cfg.MODEL,'BNFUNC')\n",
    "    normalizationLayer = layer(in_channels)\n",
    "    return normalizationLayer\n",
    "\n",
    "\n",
    "def initialize_weights(*models):\n",
    "   for model in models:\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.zero_()\n",
    "            elif isinstance(module, nn.BatchNorm2d):\n",
    "                module.weight.data.fill_(1)\n",
    "                module.bias.data.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Copyright (C) 2019 NVIDIA Corporation.  All rights reserved.\n",
    "Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.conv import _ConvNd\n",
    "from torch.nn.modules.utils import _pair\n",
    "import numpy as np\n",
    "import math\n",
    "# import network.mynn as mynn\n",
    "# import my_functionals.custom_functional as myF\n",
    "\n",
    "\n",
    "def conv2d_same(input, kernel, groups,bias=None,stride=1,padding=0,dilation=1):\n",
    "    n, c, h, w = input.shape\n",
    "    kout, ki_c_g, kh, kw = kernel.shape\n",
    "    pw = calc_pad_same(w, w, 1, kw)\n",
    "    ph = calc_pad_same(h, h, 1, kh)\n",
    "    pw_l = pw // 2\n",
    "    pw_r = pw - pw_l\n",
    "    ph_t = ph // 2\n",
    "    ph_b = ph - ph_t\n",
    "\n",
    "    input_ = F.pad(input, (pw_l, pw_r, ph_t, ph_b))\n",
    "    result = F.conv2d(input_, kernel, bias=bias, stride=stride, padding=padding, dilation=dilation, groups=groups)\n",
    "    assert result.shape == input.shape\n",
    "    return result\n",
    "\n",
    "\n",
    "class GatedSpatialConv2d(_ConvNd):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param in_channels:\n",
    "        :param out_channels:\n",
    "        :param kernel_size:\n",
    "        :param stride:\n",
    "        :param padding:\n",
    "        :param dilation:\n",
    "        :param groups:\n",
    "        :param bias:\n",
    "        \"\"\"\n",
    "\n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = _pair(stride)\n",
    "        padding = _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "        super(GatedSpatialConv2d, self).__init__(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, dilation,\n",
    "            False, _pair(0), groups, bias, 'zeros')\n",
    "\n",
    "        self._gate_conv = nn.Sequential(\n",
    "            Norm2d(in_channels+1),\n",
    "            nn.Conv2d(in_channels+1, in_channels+1, 1),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(in_channels+1, 1, 1),\n",
    "            Norm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_features, gating_features):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_features:  [NxCxHxW]  featuers comming from the shape branch (canny branch).\n",
    "        :param gating_features: [Nx1xHxW] features comming from the texture branch (resnet). Only one channel feature map.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        alphas = self._gate_conv(torch.cat([input_features, gating_features], dim=1))\n",
    "\n",
    "        input_features = (input_features * (alphas + 1)) \n",
    "        return F.conv2d(input_features, self.weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "  \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_normal_(self.weight)\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "\n",
    "\n",
    "class Conv2dPad(nn.Conv2d):\n",
    "    def forward(self, input):\n",
    "        return conv2d_same(input,self.weight,self.groups)\n",
    "\n",
    "class HighFrequencyGatedSpatialConv2d(_ConvNd):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param in_channels:\n",
    "        :param out_channels:\n",
    "        :param kernel_size:\n",
    "        :param stride:\n",
    "        :param padding:\n",
    "        :param dilation:\n",
    "        :param groups:\n",
    "        :param bias:\n",
    "        \"\"\"\n",
    "\n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride = _pair(stride)\n",
    "        padding = _pair(padding)\n",
    "        dilation = _pair(dilation)\n",
    "        super(HighFrequencyGatedSpatialConv2d, self).__init__(\n",
    "            in_channels, out_channels, kernel_size, stride, padding, dilation,\n",
    "            False, _pair(0), groups, bias)\n",
    "\n",
    "        self._gate_conv = nn.Sequential(\n",
    "            Norm2d(in_channels+1),\n",
    "            nn.Conv2d(in_channels+1, in_channels+1, 1),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(in_channels+1, 1, 1),\n",
    "            Norm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        kernel_size = 7\n",
    "        sigma = 3\n",
    "\n",
    "        x_cord = torch.arange(kernel_size).float()\n",
    "        x_grid = x_cord.repeat(kernel_size).view(kernel_size, kernel_size).float()\n",
    "        y_grid = x_grid.t().float()\n",
    "        xy_grid = torch.stack([x_grid, y_grid], dim=-1).float()\n",
    "\n",
    "        mean = (kernel_size - 1)/2.\n",
    "        variance = sigma**2.\n",
    "        gaussian_kernel = (1./(2.*math.pi*variance)) *\\\n",
    "                          torch.exp(\n",
    "                              -torch.sum((xy_grid - mean)**2., dim=-1) /\\\n",
    "                              (2*variance)\n",
    "                          )\n",
    "\n",
    "        gaussian_kernel = gaussian_kernel / torch.sum(gaussian_kernel)\n",
    "\n",
    "        gaussian_kernel = gaussian_kernel.view(1, 1, kernel_size, kernel_size)\n",
    "        gaussian_kernel = gaussian_kernel.repeat(in_channels, 1, 1, 1)\n",
    "\n",
    "        self.gaussian_filter = nn.Conv2d(in_channels=in_channels, out_channels=in_channels, padding=3,\n",
    "                                         kernel_size=kernel_size, groups=in_channels, bias=False)\n",
    "\n",
    "        self.gaussian_filter.weight.data = gaussian_kernel\n",
    "        self.gaussian_filter.weight.requires_grad = False\n",
    "\n",
    "        self.cw = nn.Conv2d(in_channels * 2, in_channels, 1)\n",
    " \n",
    "        self.procdog = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, 1),\n",
    "            Norm2d(in_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input_features, gating_features):\n",
    "        \"\"\"\n",
    "\n",
    "        :param input_features:  [NxCxHxW]  featuers comming from the shape branch (canny branch).\n",
    "        :param gating_features: [Nx1xHxW] features comming from the texture branch (resnet). Only one channel feature map.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        n, c, h, w = input_features.size()\n",
    "        smooth_features = self.gaussian_filter(input_features)\n",
    "        dog_features = input_features - smooth_features\n",
    "        dog_features = self.cw(torch.cat((dog_features, input_features), dim=1))\n",
    "        \n",
    "        alphas = self._gate_conv(torch.cat([input_features, gating_features], dim=1))\n",
    "\n",
    "        dog_features = dog_features * (alphas + 1)\n",
    "\n",
    "        return F.conv2d(dog_features, self.weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_normal_(self.weight)\n",
    "        if self.bias is not None:\n",
    "            nn.init.zeros_(self.bias)\n",
    "\n",
    "def t():\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    canny_map_filters_in = 8\n",
    "    canny_map = np.random.normal(size=(1, canny_map_filters_in, 10, 10))  # NxCxHxW\n",
    "    resnet_map = np.random.normal(size=(1, 1, 10, 10))  # NxCxHxW\n",
    "    plt.imshow(canny_map[0, 0])\n",
    "    plt.show()\n",
    "\n",
    "    canny_map = torch.from_numpy(canny_map).float()\n",
    "    resnet_map = torch.from_numpy(resnet_map).float()\n",
    "\n",
    "    gconv = GatedSpatialConv2d(canny_map_filters_in, canny_map_filters_in,\n",
    "                               kernel_size=3, stride=1, padding=1)\n",
    "    output_map = gconv(canny_map, resnet_map)\n",
    "    print('done')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = Norm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = Norm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = Norm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = Norm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = Norm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=2):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = Norm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                Norm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet18(pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet34(pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50(pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet101(pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet152(pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-152 model.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [4, 8, 36, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "def bnrelu(channels):\n",
    "    return nn.Sequential(Norm2d(channels),\n",
    "                         nn.ReLU(inplace=True))\n",
    "\n",
    "class GlobalAvgPool2d(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Global average pooling over the input's spatial dimensions\"\"\"\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        in_size = inputs.size()\n",
    "        return inputs.view((in_size[0], in_size[1], -1)).mean(dim=2)\n",
    "\n",
    "\n",
    "class IdentityResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 channels,\n",
    "                 stride=1,\n",
    "                 dilation=1,\n",
    "                 groups=1,\n",
    "                 norm_act=bnrelu,\n",
    "                 dropout=None,\n",
    "                 dist_bn=False\n",
    "                 ):\n",
    "        \"\"\"Configurable identity-mapping residual block\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channels : int\n",
    "            Number of input channels.\n",
    "        channels : list of int\n",
    "            Number of channels in the internal feature maps.\n",
    "            Can either have two or three elements: if three construct\n",
    "            a residual block with two `3 x 3` convolutions,\n",
    "            otherwise construct a bottleneck block with `1 x 1`, then\n",
    "            `3 x 3` then `1 x 1` convolutions.\n",
    "        stride : int\n",
    "            Stride of the first `3 x 3` convolution\n",
    "        dilation : int\n",
    "            Dilation to apply to the `3 x 3` convolutions.\n",
    "        groups : int\n",
    "            Number of convolution groups.\n",
    "            This is used to create ResNeXt-style blocks and is only compatible with\n",
    "            bottleneck blocks.\n",
    "        norm_act : callable\n",
    "            Function to create normalization / activation Module.\n",
    "        dropout: callable\n",
    "            Function to create Dropout Module.\n",
    "        dist_bn: Boolean\n",
    "            A variable to enable or disable use of distributed BN\n",
    "        \"\"\"\n",
    "        super(IdentityResidualBlock, self).__init__()\n",
    "        self.dist_bn = dist_bn\n",
    "\n",
    "        # Check if we are using distributed BN and use the nn from encoding.nn\n",
    "        # library rather than using standard pytorch.nn\n",
    "\n",
    "\n",
    "        # Check parameters for inconsistencies\n",
    "        if len(channels) != 2 and len(channels) != 3:\n",
    "            raise ValueError(\"channels must contain either two or three values\")\n",
    "        if len(channels) == 2 and groups != 1:\n",
    "            raise ValueError(\"groups > 1 are only valid if len(channels) == 3\")\n",
    "\n",
    "        is_bottleneck = len(channels) == 3\n",
    "        need_proj_conv = stride != 1 or in_channels != channels[-1]\n",
    "\n",
    "        self.bn1 = norm_act(in_channels)\n",
    "        if not is_bottleneck:\n",
    "            layers = [\n",
    "                (\"conv1\", nn.Conv2d(in_channels,\n",
    "                                    channels[0],\n",
    "                                    3,\n",
    "                                    stride=stride,\n",
    "                                    padding=dilation,\n",
    "                                    bias=False,\n",
    "                                    dilation=dilation)),\n",
    "                (\"bn2\", norm_act(channels[0])),\n",
    "                (\"conv2\", nn.Conv2d(channels[0], channels[1],\n",
    "                                    3,\n",
    "                                    stride=1,\n",
    "                                    padding=dilation,\n",
    "                                    bias=False,\n",
    "                                    dilation=dilation))\n",
    "            ]\n",
    "            if dropout is not None:\n",
    "                layers = layers[0:2] + [(\"dropout\", dropout())] + layers[2:]\n",
    "        else:\n",
    "            layers = [\n",
    "                (\"conv1\",\n",
    "                 nn.Conv2d(in_channels,\n",
    "                           channels[0],\n",
    "                           1,\n",
    "                           stride=stride,\n",
    "                           padding=0,\n",
    "                           bias=False)),\n",
    "                (\"bn2\", norm_act(channels[0])),\n",
    "                (\"conv2\", nn.Conv2d(channels[0],\n",
    "                                    channels[1],\n",
    "                                    3, stride=1,\n",
    "                                    padding=dilation, bias=False,\n",
    "                                    groups=groups,\n",
    "                                    dilation=dilation)),\n",
    "                (\"bn3\", norm_act(channels[1])),\n",
    "                (\"conv3\", nn.Conv2d(channels[1], channels[2],\n",
    "                                    1, stride=1, padding=0, bias=False))\n",
    "            ]\n",
    "            if dropout is not None:\n",
    "                layers = layers[0:4] + [(\"dropout\", dropout())] + layers[4:]\n",
    "        self.convs = nn.Sequential(OrderedDict(layers))\n",
    "\n",
    "        if need_proj_conv:\n",
    "            self.proj_conv = nn.Conv2d(\n",
    "                in_channels, channels[-1], 1, stride=stride, padding=0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        This is the standard forward function for non-distributed batch norm\n",
    "        \"\"\"\n",
    "        if hasattr(self, \"proj_conv\"):\n",
    "            bn1 = self.bn1(x)\n",
    "            shortcut = self.proj_conv(bn1)\n",
    "        else:\n",
    "            shortcut = x.clone()\n",
    "            bn1 = self.bn1(x)\n",
    "\n",
    "        out = self.convs(bn1)\n",
    "        out.add_(shortcut)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class WiderResNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 structure,\n",
    "                 norm_act=bnrelu,\n",
    "                 classes=0\n",
    "                 ):\n",
    "        \"\"\"Wider ResNet with pre-activation (identity mapping) blocks\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        structure : list of int\n",
    "            Number of residual blocks in each of the six modules of the network.\n",
    "        norm_act : callable\n",
    "            Function to create normalization / activation Module.\n",
    "        classes : int\n",
    "            If not `0` also include global average pooling and \\\n",
    "            a fully-connected layer with `classes` outputs at the end\n",
    "            of the network.\n",
    "        \"\"\"\n",
    "        super(WiderResNet, self).__init__()\n",
    "        self.structure = structure\n",
    "\n",
    "        if len(structure) != 6:\n",
    "            raise ValueError(\"Expected a structure with six values\")\n",
    "\n",
    "        # Initial layers\n",
    "        self.mod1 = nn.Sequential(OrderedDict([\n",
    "            (\"conv1\", nn.Conv2d(4, 64, 3, stride=1, padding=1, bias=False))\n",
    "        ]))\n",
    "\n",
    "        # Groups of residual blocks\n",
    "        in_channels = 64\n",
    "        channels = [(128, 128), (256, 256), (512, 512), (512, 1024),\n",
    "                    (512, 1024, 2048), (1024, 2048, 4096)]\n",
    "        for mod_id, num in enumerate(structure):\n",
    "            # Create blocks for module\n",
    "            blocks = []\n",
    "            for block_id in range(num):\n",
    "                blocks.append((\n",
    "                    \"block%d\" % (block_id + 1),\n",
    "                    IdentityResidualBlock(in_channels, channels[mod_id],\n",
    "                                          norm_act=norm_act)\n",
    "                ))\n",
    "\n",
    "                # Update channels and p_keep\n",
    "                in_channels = channels[mod_id][-1]\n",
    "\n",
    "            # Create module\n",
    "            if mod_id <= 4:\n",
    "                self.add_module(\"pool%d\" %\n",
    "                                (mod_id + 2), nn.MaxPool2d(3, stride=2, padding=1))\n",
    "            self.add_module(\"mod%d\" % (mod_id + 2), nn.Sequential(OrderedDict(blocks)))\n",
    "\n",
    "        # Pooling and predictor\n",
    "        self.bn_out = norm_act(in_channels)\n",
    "        if classes != 0:\n",
    "            self.classifier = nn.Sequential(OrderedDict([\n",
    "                (\"avg_pool\", GlobalAvgPool2d()),\n",
    "                (\"fc\", nn.Linear(in_channels, classes))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.mod1(img)\n",
    "        out = self.mod2(self.pool2(out))\n",
    "        out = self.mod3(self.pool3(out))\n",
    "        out = self.mod4(self.pool4(out))\n",
    "        out = self.mod5(self.pool5(out))\n",
    "        out = self.mod6(self.pool6(out))\n",
    "        out = self.mod7(out)\n",
    "        out = self.bn_out(out)\n",
    "\n",
    "        if hasattr(self, \"classifier\"):\n",
    "            out = self.classifier(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class WiderResNetA2(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 structure,\n",
    "                 norm_act=bnrelu,\n",
    "                 classes=0,\n",
    "                 dilation=False,\n",
    "                 dist_bn=False\n",
    "                 ):\n",
    "        \"\"\"Wider ResNet with pre-activation (identity mapping) blocks\n",
    "\n",
    "        This variant uses down-sampling by max-pooling in the first two blocks and \\\n",
    "         by strided convolution in the others.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        structure : list of int\n",
    "            Number of residual blocks in each of the six modules of the network.\n",
    "        norm_act : callable\n",
    "            Function to create normalization / activation Module.\n",
    "        classes : int\n",
    "            If not `0` also include global average pooling and a fully-connected layer\n",
    "            \\with `classes` outputs at the end\n",
    "            of the network.\n",
    "        dilation : bool\n",
    "            If `True` apply dilation to the last three modules and change the\n",
    "            \\down-sampling factor from 32 to 8.\n",
    "        \"\"\"\n",
    "        super(WiderResNetA2, self).__init__()\n",
    "        self.dist_bn = dist_bn\n",
    "\n",
    "        # If using distributed batch norm, use the encoding.nn as oppose to torch.nn\n",
    "\n",
    "\n",
    "        nn.Dropout = nn.Dropout2d\n",
    "        norm_act = bnrelu\n",
    "        self.structure = structure\n",
    "        self.dilation = dilation\n",
    "\n",
    "        if len(structure) != 6:\n",
    "            raise ValueError(\"Expected a structure with six values\")\n",
    "\n",
    "        # Initial layers\n",
    "        self.mod1 = torch.nn.Sequential(OrderedDict([\n",
    "            (\"conv1\", nn.Conv2d(4, 64, 3, stride=1, padding=1, bias=False))\n",
    "        ]))\n",
    "\n",
    "        # Groups of residual blocks\n",
    "        in_channels = 64\n",
    "        channels = [(128, 128), (256, 256), (512, 512), (512, 1024), (512, 1024, 2048),\n",
    "                    (1024, 2048, 4096)]\n",
    "        for mod_id, num in enumerate(structure):\n",
    "            # Create blocks for module\n",
    "            blocks = []\n",
    "            for block_id in range(num):\n",
    "                if not dilation:\n",
    "                    dil = 1\n",
    "                    stride = 2 if block_id == 0 and 2 <= mod_id <= 4 else 1\n",
    "                else:\n",
    "                    if mod_id == 3:\n",
    "                        dil = 2\n",
    "                    elif mod_id > 3:\n",
    "                        dil = 4\n",
    "                    else:\n",
    "                        dil = 1\n",
    "                    stride = 2 if block_id == 0 and mod_id == 2 else 1\n",
    "\n",
    "                if mod_id == 4:\n",
    "                    drop = partial(nn.Dropout, p=0.3)\n",
    "                elif mod_id == 5:\n",
    "                    drop = partial(nn.Dropout, p=0.5)\n",
    "                else:\n",
    "                    drop = None\n",
    "\n",
    "                blocks.append((\n",
    "                    \"block%d\" % (block_id + 1),\n",
    "                    IdentityResidualBlock(in_channels,\n",
    "                                          channels[mod_id], norm_act=norm_act,\n",
    "                                          stride=stride, dilation=dil,\n",
    "                                          dropout=drop, dist_bn=self.dist_bn)\n",
    "                ))\n",
    "\n",
    "                # Update channels and p_keep\n",
    "                in_channels = channels[mod_id][-1]\n",
    "\n",
    "            # Create module\n",
    "            if mod_id < 2:\n",
    "                self.add_module(\"pool%d\" %\n",
    "                                (mod_id + 2), nn.MaxPool2d(3, stride=2, padding=1))\n",
    "            self.add_module(\"mod%d\" % (mod_id + 2), nn.Sequential(OrderedDict(blocks)))\n",
    "\n",
    "        # Pooling and predictor\n",
    "        self.bn_out = norm_act(in_channels)\n",
    "        if classes != 0:\n",
    "            self.classifier = nn.Sequential(OrderedDict([\n",
    "                (\"avg_pool\", GlobalAvgPool2d()),\n",
    "                (\"fc\", nn.Linear(in_channels, classes))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.mod1(img)\n",
    "        out = self.mod2(self.pool2(out))\n",
    "        out = self.mod3(self.pool3(out))\n",
    "        out = self.mod4(out)\n",
    "        out = self.mod5(out)\n",
    "        out = self.mod6(out)\n",
    "        out = self.mod7(out)\n",
    "        out = self.bn_out(out)\n",
    "\n",
    "        if hasattr(self, \"classifier\"):\n",
    "            return self.classifier(out)\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "_NETS = {\n",
    "    \"16\": {\"structure\": [1, 1, 1, 1, 1, 1]},\n",
    "    \"20\": {\"structure\": [1, 1, 1, 3, 1, 1]},\n",
    "    \"38\": {\"structure\": [3, 3, 6, 3, 1, 1]},\n",
    "}\n",
    "\n",
    "__all__ = []\n",
    "for name, params in _NETS.items():\n",
    "    net_name = \"wider_resnet\" + name\n",
    "    setattr(sys.modules[__name__], net_name, partial(WiderResNet, **params))\n",
    "    __all__.append(net_name)\n",
    "for name, params in _NETS.items():\n",
    "    net_name = \"wider_resnet\" + name + \"_a2\"\n",
    "    setattr(sys.modules[__name__], net_name, partial(WiderResNetA2, **params))\n",
    "    __all__.append(net_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copyright (C) 2019 NVIDIA Corporation.  All rights reserved.\n",
    "Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n",
    "\n",
    "# Code Adapted from:\n",
    "# https://github.com/sthalles/deeplab_v3\n",
    "#\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2018 Thalles Santos Silva\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# from my_functionals import GatedSpatialConv as gsc\n",
    "\n",
    "class Crop(nn.Module):\n",
    "    def __init__(self, axis, offset):\n",
    "        super(Crop, self).__init__()\n",
    "        self.axis = axis\n",
    "        self.offset = offset\n",
    "\n",
    "    def forward(self, x, ref):\n",
    "        \"\"\"\n",
    "        :param x: input layer\n",
    "        :param ref: reference usually data in\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for axis in range(self.axis, x.dim()):\n",
    "            ref_size = ref.size(axis)\n",
    "            indices = torch.arange(self.offset, self.offset + ref_size).long()\n",
    "            indices = x.data.new().resize_(indices.size()).copy_(indices).long()\n",
    "            x = x.index_select(axis, Variable(indices))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MyIdentity(nn.Module):\n",
    "    def __init__(self, axis, offset):\n",
    "        super(MyIdentity, self).__init__()\n",
    "        self.axis = axis\n",
    "        self.offset = offset\n",
    "\n",
    "    def forward(self, x, ref):\n",
    "        \"\"\"\n",
    "        :param x: input layer\n",
    "        :param ref: reference usually data in\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return x\n",
    "\n",
    "class SideOutputCrop(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the original implementation ConvTranspose2d (fixed) and crops\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_output, kernel_sz=None, stride=None, upconv_pad=0, do_crops=True):\n",
    "        super(SideOutputCrop, self).__init__()\n",
    "        self._do_crops = do_crops\n",
    "        self.conv = nn.Conv2d(num_output, out_channels=1, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "        if kernel_sz is not None:\n",
    "            self.upsample = True\n",
    "            self.upsampled = nn.ConvTranspose2d(1, out_channels=1, kernel_size=kernel_sz, stride=stride,\n",
    "                                                padding=upconv_pad,\n",
    "                                                bias=False)\n",
    "            ##doing crops\n",
    "            if self._do_crops:\n",
    "                self.crops = Crop(2, offset=kernel_sz // 4)\n",
    "            else:\n",
    "                self.crops = MyIdentity(None, None)\n",
    "        else:\n",
    "            self.upsample = False\n",
    "\n",
    "    def forward(self, res, reference=None):\n",
    "        side_output = self.conv(res)\n",
    "        if self.upsample:\n",
    "            side_output = self.upsampled(side_output)\n",
    "            side_output = self.crops(side_output, reference)\n",
    "\n",
    "        return side_output\n",
    "\n",
    "\n",
    "class _AtrousSpatialPyramidPoolingModule(nn.Module):\n",
    "    '''\n",
    "    operations performed:\n",
    "      1x1 x depth\n",
    "      3x3 x depth dilation 6\n",
    "      3x3 x depth dilation 12\n",
    "      3x3 x depth dilation 18\n",
    "      image pooling\n",
    "      concatenate all together\n",
    "      Final 1x1 conv\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_dim, reduction_dim=256, output_stride=16, rates=[6, 12, 18]):\n",
    "        super(_AtrousSpatialPyramidPoolingModule, self).__init__()\n",
    "\n",
    "        # Check if we are using distributed BN and use the nn from encoding.nn\n",
    "        # library rather than using standard pytorch.nn\n",
    "\n",
    "        if output_stride == 8:\n",
    "            rates = [2 * r for r in rates]\n",
    "        elif output_stride == 16:\n",
    "            pass\n",
    "        else:\n",
    "            raise 'output stride of {} not supported'.format(output_stride)\n",
    "\n",
    "        self.features = []\n",
    "        # 1x1\n",
    "        self.features.append(\n",
    "            nn.Sequential(nn.Conv2d(in_dim, reduction_dim, kernel_size=1, bias=False),\n",
    "                          Norm2d(reduction_dim), nn.ReLU(inplace=True)))\n",
    "        # other rates\n",
    "        for r in rates:\n",
    "            self.features.append(nn.Sequential(\n",
    "                nn.Conv2d(in_dim, reduction_dim, kernel_size=3,\n",
    "                          dilation=r, padding=r, bias=False),\n",
    "                Norm2d(reduction_dim),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ))\n",
    "        self.features = torch.nn.ModuleList(self.features)\n",
    "\n",
    "        # img level features\n",
    "        self.img_pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        self.img_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, reduction_dim, kernel_size=1, bias=False),\n",
    "            Norm2d(reduction_dim), nn.ReLU(inplace=True))\n",
    "        self.edge_conv = nn.Sequential(\n",
    "            nn.Conv2d(1, reduction_dim, kernel_size=1, bias=False),\n",
    "            Norm2d(reduction_dim), nn.ReLU(inplace=True))\n",
    "         \n",
    "\n",
    "    def forward(self, x, edge):\n",
    "        x_size = x.size()\n",
    "\n",
    "        img_features = self.img_pooling(x)\n",
    "        img_features = self.img_conv(img_features)\n",
    "        img_features = F.interpolate(img_features, x_size[2:],\n",
    "                                     mode='bilinear',align_corners=True)\n",
    "        out = img_features\n",
    "\n",
    "        edge_features = F.interpolate(edge, x_size[2:],\n",
    "                                      mode='bilinear',align_corners=True)\n",
    "        edge_features = self.edge_conv(edge_features)\n",
    "        out = torch.cat((out, edge_features), 1)\n",
    "\n",
    "        for f in self.features:\n",
    "            y = f(x)\n",
    "            out = torch.cat((out, y), 1)\n",
    "        return out\n",
    "\n",
    "class GSCNN(nn.Module):\n",
    "    '''\n",
    "    Wide_resnet version of DeepLabV3\n",
    "    mod1\n",
    "    pool2\n",
    "    mod2 str2\n",
    "    pool3\n",
    "    mod3-7\n",
    "\n",
    "      structure: [3, 3, 6, 3, 1, 1]\n",
    "      channels = [(128, 128), (256, 256), (512, 512), (512, 1024), (512, 1024, 2048),\n",
    "                  (1024, 2048, 4096)]\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_classes = 2, trunk=None, criterion=None):\n",
    "        \n",
    "        super(GSCNN, self).__init__()\n",
    "        self.criterion = criterion\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        wide_resnet = wider_resnet38_a2(classes=2, dilation=True)\n",
    "        wide_resnet = torch.nn.DataParallel(wide_resnet)\n",
    "        \n",
    "        wide_resnet = wide_resnet.module\n",
    "        self.mod1 = wide_resnet.mod1\n",
    "        self.mod2 = wide_resnet.mod2\n",
    "        self.mod3 = wide_resnet.mod3\n",
    "        self.mod4 = wide_resnet.mod4\n",
    "        self.mod5 = wide_resnet.mod5\n",
    "        self.mod6 = wide_resnet.mod6\n",
    "        self.mod7 = wide_resnet.mod7\n",
    "        self.pool2 = wide_resnet.pool2\n",
    "        self.pool3 = wide_resnet.pool3\n",
    "        self.interpolate = F.interpolate\n",
    "        del wide_resnet\n",
    "\n",
    "        self.dsn1 = nn.Conv2d(64, 1, 1)\n",
    "        self.dsn3 = nn.Conv2d(256, 1, 1)\n",
    "        self.dsn4 = nn.Conv2d(512, 1, 1)\n",
    "        self.dsn7 = nn.Conv2d(4096, 1, 1)\n",
    "\n",
    "        self.res1 = BasicBlock(64, 64, stride=1, downsample=None)\n",
    "        self.d1 = nn.Conv2d(64, 32, 1)\n",
    "        self.res2 = BasicBlock(32, 32, stride=1, downsample=None)\n",
    "        self.d2 = nn.Conv2d(32, 16, 1)\n",
    "        self.res3 = BasicBlock(16, 16, stride=1, downsample=None)\n",
    "        self.d3 = nn.Conv2d(16, 8, 1)\n",
    "        self.fuse = nn.Conv2d(8, 1, kernel_size=1, padding=0, bias=False)\n",
    "\n",
    "        self.cw = nn.Conv2d(2, 1, kernel_size=1, padding=0, bias=False)\n",
    "\n",
    "        self.gate1 = GatedSpatialConv2d(32, 32)\n",
    "        self.gate2 = GatedSpatialConv2d(16, 16)\n",
    "        self.gate3 = GatedSpatialConv2d(8, 8)\n",
    "         \n",
    "        self.aspp = _AtrousSpatialPyramidPoolingModule(4096, 256,\n",
    "                                                       output_stride=8)\n",
    "\n",
    "        self.bot_fine = nn.Conv2d(128, 48, kernel_size=1, bias=False)\n",
    "        self.bot_aspp = nn.Conv2d(1280 + 256, 256, kernel_size=1, bias=False)\n",
    "\n",
    "        self.final_seg = nn.Sequential(\n",
    "            nn.Conv2d(256 + 48, 256, kernel_size=3, padding=1, bias=False),\n",
    "            Norm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n",
    "            Norm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, num_classes, kernel_size=1, bias=False))\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        initialize_weights(self.final_seg)\n",
    "\n",
    "    def forward(self, inp, gts=None):\n",
    "\n",
    "        x_size = inp.size() \n",
    "\n",
    "        # res 1\n",
    "        m1 = self.mod1(inp)\n",
    "        print(\"m1 passed.\")\n",
    "        # res 2\n",
    "        m2 = self.mod2(self.pool2(m1))\n",
    "        print(\"m2 passed.\")\n",
    "        # res 3\n",
    "        m3 = self.mod3(self.pool3(m2))\n",
    "        print(\"m3 passed.\")\n",
    "        # res 4-7\n",
    "        m4 = self.mod4(m3)\n",
    "        print(\"m4 passed.\")\n",
    "        m5 = self.mod5(m4)\n",
    "        print(\"m5 passed.\")\n",
    "        m6 = self.mod6(m5)\n",
    "        print(\"m6 passed.\")\n",
    "        m7 = self.mod7(m6) \n",
    "        print(\"m7 passed.\")\n",
    "\n",
    "        s3 = F.interpolate(self.dsn3(m3), x_size[2:],\n",
    "                            mode='bilinear', align_corners=True)\n",
    "        s4 = F.interpolate(self.dsn4(m4), x_size[2:],\n",
    "                            mode='bilinear', align_corners=True)\n",
    "        s7 = F.interpolate(self.dsn7(m7), x_size[2:],\n",
    "                            mode='bilinear', align_corners=True)\n",
    "        \n",
    "        m1f = F.interpolate(m1, x_size[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "        im_arr = inp.cpu().numpy().transpose((0,2,3,1)).astype(np.uint8)\n",
    "        canny = np.zeros((x_size[0], 1, x_size[2], x_size[3]))\n",
    "        for i in range(x_size[0]):\n",
    "            canny[i] = cv2.Canny(im_arr[i],10,100)\n",
    "        canny = torch.from_numpy(canny).float()\n",
    "\n",
    "        cs = self.res1(m1f)\n",
    "        cs = F.interpolate(cs, x_size[2:],\n",
    "                           mode='bilinear', align_corners=True)\n",
    "        cs = self.d1(cs)\n",
    "        cs = self.gate1(cs, s3)\n",
    "        cs = self.res2(cs)\n",
    "        cs = F.interpolate(cs, x_size[2:],\n",
    "                           mode='bilinear', align_corners=True)\n",
    "        cs = self.d2(cs)\n",
    "        cs = self.gate2(cs, s4)\n",
    "        cs = self.res3(cs)\n",
    "        cs = F.interpolate(cs, x_size[2:],\n",
    "                           mode='bilinear', align_corners=True)\n",
    "        cs = self.d3(cs)\n",
    "        cs = self.gate3(cs, s7)\n",
    "        cs = self.fuse(cs)\n",
    "        cs = F.interpolate(cs, x_size[2:],\n",
    "                           mode='bilinear', align_corners=True)\n",
    "        edge_out = self.sigmoid(cs)\n",
    "        cat = torch.cat((edge_out, canny), dim=1)\n",
    "        acts = self.cw(cat)\n",
    "        acts = self.sigmoid(acts)\n",
    "\n",
    "        # aspp\n",
    "        x = self.aspp(m7, acts)\n",
    "        dec0_up = self.bot_aspp(x)\n",
    "\n",
    "        dec0_fine = self.bot_fine(m2)\n",
    "        dec0_up = self.interpolate(dec0_up, m2.size()[2:], mode='bilinear',align_corners=True)\n",
    "        dec0 = [dec0_fine, dec0_up]\n",
    "        dec0 = torch.cat(dec0, 1)\n",
    "\n",
    "        dec1 = self.final_seg(dec0)  \n",
    "        seg_out = self.interpolate(dec1, x_size[2:], mode='bilinear')            \n",
    "       \n",
    "#         if self.training:\n",
    "#             return self.criterion((seg_out, edge_out), gts)              \n",
    "#         else:\n",
    "        return seg_out, edge_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shimash/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m1 passed.\n",
      "m2 passed.\n",
      "m3 passed.\n",
      "m4 passed.\n",
      "m5 passed.\n",
      "m6 passed.\n",
      "m7 passed.\n"
     ]
    }
   ],
   "source": [
    "net = GSCNN()\n",
    "loss_dict = None\n",
    "loss_dict, edge_out = net(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(loss_dict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(edge_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(eval_mask_boundary(loss_dict.detach().numpy(), mask.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17185"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(seg2bmap(mask.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
